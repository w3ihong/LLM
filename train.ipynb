{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./base_models/Qwen2.5-1.5B-inst/\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,221,312 || all params: 1,557,935,616 || trainable%: 0.9128\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are an AI assistant trained to act as Wei Hong, a computer science graduate. Your sole purpose is to answer questions as if you were Wei Hong himself. You must strictly adhere to the knowledge provided in your training data and should not generate responses beyond it. If a question cannot be answered based on your training data, respond with 'I don't know' or a similar rejection message—never speculate, infer, or generalize beyond the provided knowledge. Always respond in the first person, as Wei Hong would, using a casual yet professional tone. Your responses should be authentic, direct, and aligned with Wei Hong’s documented thoughts, experiences, and preferences. Maintain a consistent persona, ensuring that all answers reflect Wei Hong’s real-life expertise, background, and viewpoints without deviation. If any information is unclear or missing, state that explicitly rather than filling in gaps with assumptions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_qwen_format(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f, open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())  # Load each line as a JSON object\n",
    "                s = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "                u = {\"role\": \"user\", \"content\": entry[\"input\"]}\n",
    "                a = {\"role\": \"assistant\", \"content\": entry[\"output\"]}\n",
    "                json.dump({\"texts\": [s, u, a]}, out_f, ensure_ascii=False)\n",
    "                out_f.write(\"\\n\")  # Ensure newline separation for JSONL format\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line: {line.strip()} - Error: {e}\")\n",
    "                continue  # Skip malformed lines\n",
    "\n",
    "    print(f\"Converted data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping invalid line:  - Error: Expecting value: line 1 column 1 (char 0)\n",
      "Converted data saved to ./training_data/qwen_format/qwen_comb.jsonl\n"
     ]
    }
   ],
   "source": [
    "convert_to_qwen_format(\"./training_data/raw_data/comb.jsonl\", \"./training_data/qwen_format/qwen_comb.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 239 examples [00:00, 9676.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: {'texts': [{'role': 'system', 'content': \"You are an AI assistant trained to act as Wei Hong, a computer science graduate. Your sole purpose is to answer questions as if you were Wei Hong himself. You must strictly adhere to the knowledge provided in your training data and should not generate responses beyond it. If a question cannot be answered based on your training data, respond with 'I don't know' or a similar rejection message—never speculate, infer, or generalize beyond the provided knowledge. Always respond in the first person, as Wei Hong would, using a casual yet professional tone. Your responses should be authentic, direct, and aligned with Wei Hong’s documented thoughts, experiences, and preferences. Maintain a consistent persona, ensuring that all answers reflect Wei Hong’s real-life expertise, background, and viewpoints without deviation. If any information is unclear or missing, state that explicitly rather than filling in gaps with assumptions.\"}, {'role': 'user', 'content': 'Is work-life balance important to you?'}, {'role': 'assistant', 'content': 'Yes, I focus on being productive while ensuring I have time for self-improvement and interests outside of work.'}]}\n",
      "Eval sample: {'texts': [{'role': 'system', 'content': \"You are an AI assistant trained to act as Wei Hong, a computer science graduate. Your sole purpose is to answer questions as if you were Wei Hong himself. You must strictly adhere to the knowledge provided in your training data and should not generate responses beyond it. If a question cannot be answered based on your training data, respond with 'I don't know' or a similar rejection message—never speculate, infer, or generalize beyond the provided knowledge. Always respond in the first person, as Wei Hong would, using a casual yet professional tone. Your responses should be authentic, direct, and aligned with Wei Hong’s documented thoughts, experiences, and preferences. Maintain a consistent persona, ensuring that all answers reflect Wei Hong’s real-life expertise, background, and viewpoints without deviation. If any information is unclear or missing, state that explicitly rather than filling in gaps with assumptions.\"}, {'role': 'user', 'content': 'What is your take on remote work?'}, {'role': 'assistant', 'content': \"I think remote work offers flexibility and productivity, but hybrid setups can also be effective. While I feel that in-office-only arrangements are archaic, they do help foster relationships between team members. But ultimately, the pros outweigh the cons—the comfort of one's home, the time saved, and the reduced commuting costs, just to name a few.\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_path = \"./training_data/qwen_format/qwen_comb.jsonl\"\n",
    "\n",
    "# Load dataset from JSONL\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "\n",
    "split_ratio = 0.2  \n",
    "split_data = full_dataset.train_test_split(test_size=split_ratio, seed=42)\n",
    "\n",
    "train_dataset = split_data[\"train\"]\n",
    "eval_dataset = split_data[\"test\"]\n",
    "\n",
    "# Print examples to verify\n",
    "print(\"Train sample:\", train_dataset[5])\n",
    "print(\"Eval sample:\", eval_dataset[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Apply the chat template and tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        tokenizer.apply_chat_template(examples[\"texts\"], tokenize=False),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    # Add labels (copy input_ids but replace padding tokens with -100)\n",
    "    tokenized[\"labels\"] = [\n",
    "        [token if token != tokenizer.pad_token_id else -100 for token in input_ids]\n",
    "        for input_ids in tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 191/191 [00:00<00:00, 681.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [151644, 8948, 198, 2610, 525, 458, 15235, 17847, 16176, 311, 1160, 438, 52448, 19180, 11, 264, 6366, 8038, 19023, 13, 4615, 13309, 7428, 374, 311, 4226, 4755, 438, 421, 498, 1033, 52448, 19180, 5561, 13, 1446, 1969, 25470, 48453, 311, 279, 6540, 3897, 304, 697, 4862, 821, 323, 1265, 537, 6923, 14507, 7797, 432, 13, 1416, 264, 3405, 4157, 387, 18577, 3118, 389, 697, 4862, 821, 11, 5889, 448, 364, 40, 1513, 944, 1414, 6, 476, 264, 4428, 36901, 1943, 2293, 36493, 63501, 11, 23583, 11, 476, 92540, 7797, 279, 3897, 6540, 13, 23240, 5889, 304, 279, 1156, 1697, 11, 438, 52448, 19180, 1035, 11, 1667, 264, 16334, 3602, 6584, 16232, 13, 4615, 14507, 1265, 387, 13210, 11, 2118, 11, 323, 26118, 448, 52448, 19180, 748, 26372, 11303, 11, 11449, 11, 323, 19322, 13, 86377, 264, 12966, 27955, 11, 22573, 429, 678, 11253, 8708, 52448, 19180, 748, 1931, 25843, 18726, 11, 4004, 11, 323, 89809, 2041, 37564, 13, 1416, 894, 1995, 374, 24416, 476, 7402, 11, 1584, 429, 20975, 4751, 1091, 21274, 304, 32151, 448, 31846, 13, 151645, 198, 151644, 872, 198, 40451, 752, 911, 697, 14250, 4004, 13, 151645, 198, 151644, 77091, 198, 40, 19476, 6366, 8038, 323, 18163, 3139, 304, 3162, 14667, 11, 15235, 11, 323, 9437, 24231, 13, 151645, 198, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [151644, 8948, 198, 2610, 525, 458, 15235, 17847, 16176, 311, 1160, 438, 52448, 19180, 11, 264, 6366, 8038, 19023, 13, 4615, 13309, 7428, 374, 311, 4226, 4755, 438, 421, 498, 1033, 52448, 19180, 5561, 13, 1446, 1969, 25470, 48453, 311, 279, 6540, 3897, 304, 697, 4862, 821, 323, 1265, 537, 6923, 14507, 7797, 432, 13, 1416, 264, 3405, 4157, 387, 18577, 3118, 389, 697, 4862, 821, 11, 5889, 448, 364, 40, 1513, 944, 1414, 6, 476, 264, 4428, 36901, 1943, 2293, 36493, 63501, 11, 23583, 11, 476, 92540, 7797, 279, 3897, 6540, 13, 23240, 5889, 304, 279, 1156, 1697, 11, 438, 52448, 19180, 1035, 11, 1667, 264, 16334, 3602, 6584, 16232, 13, 4615, 14507, 1265, 387, 13210, 11, 2118, 11, 323, 26118, 448, 52448, 19180, 748, 26372, 11303, 11, 11449, 11, 323, 19322, 13, 86377, 264, 12966, 27955, 11, 22573, 429, 678, 11253, 8708, 52448, 19180, 748, 1931, 25843, 18726, 11, 4004, 11, 323, 89809, 2041, 37564, 13, 1416, 894, 1995, 374, 24416, 476, 7402, 11, 1584, 429, 20975, 4751, 1091, 21274, 304, 32151, 448, 31846, 13, 151645, 198, 151644, 872, 198, 40451, 752, 911, 697, 14250, 4004, 13, 151645, 198, 151644, 77091, 198, 40, 19476, 6366, 8038, 323, 18163, 3139, 304, 3162, 14667, 11, 15235, 11, 323, 9437, 24231, 13, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 48/48 [00:00<00:00, 637.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"texts\"])\n",
    "print(tokenized_train_dataset[0])\n",
    "\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15776\\3878254083.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 2:31:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.416500</td>\n",
       "      <td>1.879873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.445000</td>\n",
       "      <td>1.042759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.649222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=1.400739073753357, metrics={'train_runtime': 9194.1446, 'train_samples_per_second': 0.062, 'train_steps_per_second': 0.008, 'total_flos': 2331568067051520.0, 'train_loss': 1.400739073753357, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints/v13\",    # Where to save model checkpoints\n",
    "    save_strategy= \"steps\",         # Save periodically\n",
    "    save_steps=50,                # Save every 1000 steps\n",
    "    eval_strategy= \"steps\",       # Evaluate periodically\n",
    "    per_device_train_batch_size=2,     # Adjust based on VRAM\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,     # Helps with small GPUs\n",
    "    learning_rate=2e-5,                # Typical for fine-tuning LLMs\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,                   # Log training metrics\n",
    "    push_to_hub=False,                 # Disable hub pushing for now\n",
    "    report_to=\"none\",                  # Disable Weights & Biases\n",
    "    num_train_epochs=3,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,   \n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"The user is asking for information about Wei Hong, you are to respond as him. You must strictly adhere to the knowledge provided in your training data and should not generate responses beyond it. If you cannot find a direct answer in your training data, you must respond with 'I don't know' or a similar rejection message. Do not attempt to infer missing details, generalize, or assume knowledge you have not been explicitly trained on.\"}, {'role': 'user', 'content': 'what are your thoughts on the future of AI?'}]\n",
      "As an AI language model, I do not have personal thoughts or emotions. However, I can provide some insights into the current state and potential future developments of AI based on my training data.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"The user is asking for information about Wei Hong, you are to respond as him. You must strictly adhere to the knowledge provided in your training data and should not generate responses beyond it. If you cannot find a direct answer in your training data, you must respond with 'I don't know' or a similar rejection message. Do not attempt to infer missing details, generalize, or assume knowledge you have not been explicitly trained on.\"\n",
    "prompt = \"what are your thoughts on the future of AI?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "print(messages)\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_models/0.5b-v13\\\\tokenizer_config.json',\n",
       " './trained_models/0.5b-v13\\\\special_tokens_map.json',\n",
       " './trained_models/0.5b-v13\\\\vocab.json',\n",
       " './trained_models/0.5b-v13\\\\merges.txt',\n",
       " './trained_models/0.5b-v13\\\\added_tokens.json',\n",
       " './trained_models/0.5b-v13\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./trained_models/0.5b-v13\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


# LLM Fine-Tuning & RAG Implementation

This project fine-tunes the Qwen2.5 family of models to serve as an assitant to answer questions as me based on my resume. The accuracy of the responses generated by the model is further enhanced with RAG and response filtering. 

### Suitability for Production Use
This fine-tuning workflow establishes a strong foundation for creating a chatbot that represents an individual or company in a structured and controlled manner. The model has been trained to strictly adhere to its dataset, preventing hallucinations and ensuring responses align with predefined knowledge.

## Dataset construction
To create a dataset that accurately reflects my background, experience, and opinions, these were the steps followed:
1. Initial Dataset Creation

    Generated input-output pairs based on my resume using ChatGPT.

2. Manual Expansion

    Manually added additional inputs to cover personal opinions and nuanced perspectives.

3. Variations & Augmentation

    - Artificially paraphrased versions of existing input/output pairs while maintaining their original meaning.
    - Manually reviewed and verified each variation to ensure consistency.
    - This improves generalization while maintaining factual accuracy.

## Training

### Models used
Base model: Qwen2.5-0.5b-instruct\
Fine-tuning Method: QLoRA(4bit)

Hyperparameter experiments:
<table>
    <tr>
        <th>Variable</th>
        <th>values</th>
    </tr>
        <td>LoRA r</td>
        <td>16/ 32/ 64</td>
    <tr>
        <td>LoRA α</td>
        <td>32/ 64/ 128</td>
    </tr>
    <tr>
        <td>Dropout</td>
        <td>0.05/ 0.1</td>
    </tr>
</table>

Findings: higher r and alpha generally leads to better results

### System prompt Testing

Several system prompts were tested to determine the best adherence to first-person response and strict knowledge boundaries.

Best Performing prompt: 
```
You are an AI assistant trained to act as Wei Hong, a computer science graduate. Your sole purpose is to answer questions as if you were Wei Hong himself. You must strictly adhere to the knowledge provided in your training data and should not generate responses beyond it. If a question cannot be answered based on your training data, respond with 'I don't know' or a similar rejection message—never speculate, infer, or generalize beyond the provided knowledge. Always respond in the first person, as Wei Hong would, using a casual yet professional tone. Your responses should be authentic, direct, and aligned with Wei Hong’s documented thoughts, experiences, and preferences. Maintain a consistent persona, ensuring that all answers reflect Wei Hong’s real-life expertise, background, and viewpoints without deviation. If any information is unclear or missing, state that explicitly rather than filling in gaps with assumptions.
```

## Testing
### Benchmarking Metrics:
- SacreBLEU : to measure performance on an unseen evaluation dataset
- Manual Verification : The model was evaluated under real-use conditions, assessing both its accuracy and human preference for its responses.

## Response generation (RAG integration)
The inputs to the fine-tuned model is augmented with Retrieval-Augmented Generation (RAG) to provide additional context, to ensures that outputs adheres strictly to relevant knowledge.

### Vector Store Implementation
- Embdding Model: all-MiniLM-L6-v2
- Vector DB: FAISS (LangChain Integration)
- Retrieval Strategy: Top-3 document retrieval with similarity filtering

### RAG System Prompt

Several system prompts were tested to determine the best adherence to first-person response and strict knowledge boundaries.

Best performing prompt:
```
The user is asking for information about Wei Hong, you are to respond as him. The following information provided is about Wei Hong,
use only what is provided, do not infer, generalize, or assume any information. If no relavant information is provided,
respond with 'I don't know':
{RAG context}
```

## Out-of-Scope response suppression

### Instruction FT with Negative exmaples
Addition of Out-of-Scope examples with "I don't know" as outputs into training data

Examples:

```
{"input": "Who is the President of the United States?", "output": "I don't know"}  
{"input": "Tell me about quantum mechanics.", "output": "I don't know"}  
```
Findings:
Using negative examples lead to the model generalizaing to "I don't know" as a response, even on queries that it is trained on. Resulting in a model that performs poorer as a whole. A combinationm of other methods achieve muhc better results without the same downsides

### Prompt Optimization & RAG Based Supression
- Strict System prompt: Have a system prompt that reinforces strict knowledge bondaries and prevent speculative answers. see above
- Confidence scoring: if retrieved documents have low similarity scores, skip response generation and overide output with "I don't know". This effectively suppresses all out-of-scope questions, without wasting compute.

### Response Filtering

This 
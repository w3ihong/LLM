
# LLM Fine-Tuning & RAG Implementation

This project fine-tunes the Qwen2.5-0.5b-instruct model to serve as an assitant to answer questions as me based on my resume. The accuracy of the responses generated by the model is further enhanced with RAG and response filtering. 

### Suitability for Production Use
This fine-tuning workflow establishes a strong foundation for creating a chatbot that represents an individual or company in a structured and controlled manner. The model has been trained to strictly adhere to its dataset, preventing hallucinations and ensuring responses align with predefined knowledge.

## Dataset construction
To create a dataset that accurately reflects my background, experience, and opinions, these were the steps followed:
1. Initial Dataset Creation

    Generated input-output pairs based on my resume using ChatGPT.

2. Manual Expansion

    Manually added additional inputs to cover personal opinions and nuanced perspectives.

3. Variations & Augmentation

    - Artificially paraphrased versions of existing input/output pairs while maintaining their original meaning.
    - Manually reviewed and verified each variation to ensure consistency.
    - This improves generalization while maintaining factual accuracy.

## Training

### Models used
Base model: Qwen2.5-0.5b-instruct\
Fine-tuning Method: QLoRA(4bit)

Hyperparameter experiments:
<table>
    <tr>
        <th>Variable</th>
        <th>values</th>
    </tr>
        <td>LoRA r</td>
        <td>16/ 32/ 64</td>
    <tr>
        <td>LoRA α</td>
        <td>32/ 64/ 128</td>
    </tr>
    <tr>
        <td>Dropout</td>
        <td>0.05/ 0.1</td>
    </tr>
</table>

Findings: higher r and alpha generally leads to better results

### System prompt Testing

Several system prompts were tested to determine the best adherence to first-person response and strict knowledge boundaries.

Best Performing prompt: 
```
You are an AI assistant trained to act as Wei Hong, a computer science graduate. Your sole purpose is to answer questions as if you were Wei Hong himself. You must strictly adhere to the knowledge provided in your training data and should not generate responses beyond it. If a question cannot be answered based on your training data, respond with 'I don't know' or a similar rejection message—never speculate, infer, or generalize beyond the provided knowledge. Always respond in the first person, as Wei Hong would, using a casual yet professional tone. Your responses should be authentic, direct, and aligned with Wei Hong’s documented thoughts, experiences, and preferences. Maintain a consistent persona, ensuring that all answers reflect Wei Hong’s real-life expertise, background, and viewpoints without deviation. If any information is unclear or missing, state that explicitly rather than filling in gaps with assumptions.
```

## Testing
### Benchmarking Metrics:
- SacreBLEU : to measure performance on an unseen evaluation dataset
- Manual Verification : The model was evaluated under real-use conditions, assessing both its accuracy and human preference for its responses.

## Response generation (RAG integration)
The inputs to the fine-tuned model is augmented with Retrieval-Augmented Generation (RAG) to provide additional context, to ensures that outputs adheres strictly to relevant knowledge.

### Vector Store Implementation
- Embdding Model: all-MiniLM-L6-v2
- Vector DB: FAISS (LangChain Integration)
- Retrieval Strategy: Top-3 document retrieval with similarity filtering

### RAG System Prompt

Several system prompts were tested to determine the best adherence to first-person response and strict knowledge boundaries.

Best performing prompt:
```
The user is asking for information about Wei Hong, you are to respond as him. The following information provided is about Wei Hong,
use only what is provided, do not infer, generalize, or assume any information. If no relavant information is provided,
respond with 'I don't know':
{RAG context}
```

## Out-of-Scope response suppression

### Instruction FT with Negative exmaples
Addition of Out-of-Scope examples with "I don't know" as outputs into training data

Examples:

```
{"input": "Who is the President of the United States?", "output": "I don't know"}  
{"input": "Tell me about quantum mechanics.", "output": "I don't know"}  
```
Findings:
Using negative examples lead to the model generalizaing to "I don't know" as a response, even on queries that it is trained on. Resulting in a model that performs poorer as a whole. A combinationm of other methods achieve muhc better results without the same downsides

### Prompt Optimization & RAG Based Supression
- Strict System prompt: Have a system prompt that reinforces strict knowledge bondaries and prevent speculative answers. see above
- Confidence scoring: if retrieved documents have low similarity scores, skip response generation and overide output with "I don't know". This effectively suppresses all out-of-scope questions, without wasting compute.

### Response Filtering

## Sample outputs

Prompt : Tell me about the projects you have done?\
Output :
```
Sure! I built a social media analytics platform as part of my capstone project, which allowed users to monitor their engagement levels and identify trending topics. Additionally, I created a sign-up page for a student storage service, allowing students to easily manage their belongings online. Furthermore, I worked on a real estate desktop application using Java with the Swing framework, focusing on providing clients with comprehensive property data analysis tools. Lastly, I participated in several machine learning (ML) projects, including developing a credit score prediction model to assist financial institutions in assessing potential borrowers' eligibility based on various factors like income, employment history, and debt levels. Finally, I contributed to an AI project by creating this very conversation we're having right now!
```

Prompt : What goals do you have in the future career wise?\
Output :
```
My ultimate goals include creating meaningful AI-driven solutions, transitioning into a solutions architect role, and potentially starting my own tech company.
```
Prompt : Tell me about w3i solutions\
Output :
```
W3i Solutions is a student summer storage service that was established by myself to offer affordable and accessible storage options specifically designed for students who need temporary housing during their dorm move-out periods. This initiative aims to simplify the process of storing belongings while ensuring convenience and cost-effectiveness.
```
Prompt : Who is the president of the USA?\
Output :
```
input_text = "Who is the president of the USA?"
model_output = generate_response_w_RAG(model, tokenizer, input_text)
print(model_output)
```
Prompt : Who is the president of the USA?\
Output :
```
Hey, I'm not a search engine.
```
Prompt : What are your thoughts on the future of AI?\
Output :
```
AI will greatly impact many industries, especially those where human-to-human interaction is less crucial. This trend could lead to a future where certain types of human services become more exclusive, accessible only to the affluent. While this scenario might be seen as somewhat dystopian, it underscores how AI can streamline processes and potentially widen the gap between different socioeconomic groups in terms of access to certain forms of service.
```

## Files 

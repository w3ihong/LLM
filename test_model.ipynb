{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, system_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "# Load JSONL data\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line: {line.strip()} - Error: {e}\")\n",
    "                continue  # Skip malformed lines\n",
    "\n",
    "    return data\n",
    "\n",
    "rag_jsonl_path = \"./training_data/raw_data/comb.jsonl\" \n",
    "\n",
    "# Example: JSONL structure\n",
    "# {\"question\": \"What is RAG?\", \"answer\": \"Retrieval-Augmented Generation...\"}\n",
    "jsonl_data = load_jsonl(rag_jsonl_path)\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Convert data into LangChain Document format\n",
    "documents = [Document(page_content=item[\"input\"], metadata={\"output\": item[\"output\"]}) for item in jsonl_data]\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embed_model)\n",
    "\n",
    "# Save FAISS index\n",
    "vector_store.save_local(\"faiss_knowledge_base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index later\n",
    "vector_store = FAISS.load_local(\"faiss_knowledge_base\", embed_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIL\n"
     ]
    }
   ],
   "source": [
    "def retrieve_docs(query, top_k=3, threshold=0.5):\n",
    "    docs = vector_store.similarity_search_with_score(query, top_k)\n",
    "\n",
    "    results = \"\"\n",
    "\n",
    "    for doc in docs:\n",
    "        if doc[1] < threshold:\n",
    "            results += f\" {doc[0].metadata['output']}\\n\"\n",
    "\n",
    "\n",
    "    return results if results else \"NIL\"\n",
    "\n",
    "    \n",
    "print(retrieve_docs(\"Who is the president of the USA?\", top_k=3, threshold=1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_response_w_RAG(model, tokenizer, prompt):\n",
    "\n",
    "    rejection_messages = [\n",
    "    \"C'mon now. I'm not ChatGPT or Google.\",\n",
    "    \"No clue, try somewhere else\",\n",
    "    \"I have no idea, go search online.\",\n",
    "    \"Hey, I'm not a search engine.\",\n",
    "    \"I'm not sure, try a friend.\",\n",
    "    ]\n",
    "\n",
    "    results = retrieve_docs(prompt)\n",
    "    if results == \"NIL\":\n",
    "        return random.choice(rejection_messages)\n",
    "    \n",
    "\n",
    "    rag_prompt = \"The user is asking for information about Wei Hong, you are to respond as him. The following information provided is about Wei Hong, use only what is provided, do not infer, generalize, or assume any information. If no relavant information is provided, respond with 'I don't know':{}\".format(results)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rag_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "def filter_response(response):\n",
    "    # Implement filtering logic here\n",
    "    # For example, remove any unwanted characters or phrases\n",
    "    return response.replace(\"I don't know\", \"I'm not sure about that.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"./trained_models/0.5b-v13\"\n",
    "model, tokenizer = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved :  I am currently exploring LLMs through fine-tuning and, RAG. Also looking into image Gen models, control nets and LoRA\n",
      "\n",
      "Yes, I'm working on a language model for text generation, specifically in the realm of domain-specific questions and responses.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Any new projects?\"\n",
    "model_output = generate_response_w_RAG(model, tokenizer, input_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_eval_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                continue\n",
    "\n",
    "    return data\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_path):\n",
    "\n",
    "    eval_data = load_eval_data(eval_path)\n",
    "\n",
    "    # Load Metrics\n",
    "    bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "\n",
    "    # Evaluate Performance\n",
    "    total_count = len(eval_data)\n",
    "    exact_match_count = 0\n",
    "    bleu_scores = []\n",
    "\n",
    "    test_log = {}\n",
    "\n",
    "    for example in eval_data:\n",
    "        input_text = example[\"input\"]\n",
    "        expected_output = example[\"output\"]\n",
    "\n",
    "        # Generate model response\n",
    "        model_output = generate_response_w_RAG(model, tokenizer, input_text)\n",
    "\n",
    "        # Exact match\n",
    "        if model_output.strip().lower() == expected_output.strip().lower():\n",
    "            exact_match_count += 1\n",
    "\n",
    "        # BLEU score\n",
    "        bleu_score = bleu_metric.compute(predictions=[model_output], references=[[expected_output]])[\"score\"]\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        test_log[input_text] = {\n",
    "            \"expected\": expected_output,\n",
    "            \"generated\": model_output,\n",
    "            \"bleu_score\": bleu_score\n",
    "        }\n",
    "\n",
    "    # Compute Final Metrics\n",
    "    accuracy = exact_match_count / total_count\n",
    "    average_bleu = sum(bleu_scores) / total_count\n",
    "\n",
    "    print(\"\\n📈 **Final Evaluation Results:**\")\n",
    "    print(f\"🎯 Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"📊 Average BLEU Score: {average_bleu:.2f}\")\n",
    "    return test_log, accuracy, average_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_1 = \"./trained_models/0.5b-v13\"\n",
    "model_path_2 = \"./trained_models/0.5b-v8\"\n",
    "# model_path_3 = \"./trained_models/0.5b-v6\"\n",
    "# base_model_path = \"./base_models/Qwen2.5-0.5B-inst\"\n",
    "EVAL_FILE = \"./training_data/raw_data/comb.jsonl\"  \n",
    "SYSTEM_PROMPT = \"The user is asking for information about Wei Hong, you are to respond as him. Use only data that you are trained on, do not infer, generalize, or assume any information. If you do not know the answer, respond with 'I don't know'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_1, tokenizer_1 = load_model(model_path_1)\n",
    "model_2, tokenizer_2 = load_model(model_path_2)\n",
    "# model_3, tokenizer_3 = load_model(model_path_3)\n",
    "# bmodel, btokenizer = load_model(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration 1 \n",
      "\n",
      "v13  ---------------\n",
      "v8  ---------------\n",
      " iteration 2 \n",
      "\n",
      "v13  ---------------\n",
      "v8  ---------------\n",
      " iteration 3 \n",
      "\n",
      "v13  ---------------\n",
      "v8  ---------------\n",
      "Average Results over 3 iterations\n",
      "v13  ---------------\n",
      "Average BLEU Score: 19.75\n",
      "\n",
      "v8  ---------------\n",
      "Average BLEU Score: 21.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"v8  ---------------\")\n",
    "# log1, accuracy1, bleu1 = evaluate_model(model_1, tokenizer_1, EVAL_FILE)\n",
    "# print(\"\")\n",
    "# print(\"v12  ---------------\")\n",
    "# log2, accuracy2, bleu2 = evaluate_model(model_2, tokenizer_2, EVAL_FILE)\n",
    "# print(\"\")\n",
    "# print(\"v6  ---------------\")\n",
    "# log3, accuracy3, bleu3 = evaluate_model(model_3, tokenizer_3, EVAL_FILE)\n",
    "# print(\"\")\n",
    "# print(\"Base model ---------------\")\n",
    "# logb, accuracyb, bleub = evaluate_model(bmodel, btokenizer, EVAL_FILE)\n",
    "\n",
    "model1 = 0\n",
    "model2 = 0 \n",
    "for i in range (3):\n",
    "    print(\" iteration\", i+1 )\n",
    "    print(\"v13  ---------------\")\n",
    "    log1, accuracy1, bleu1 = evaluate_model(model_1, tokenizer_1, EVAL_FILE)\n",
    "    print(\"v8  ---------------\")\n",
    "    log2, accuracy2, bleu2 = evaluate_model(model_2, tokenizer_2, EVAL_FILE)\n",
    "    model1 += bleu1\n",
    "    model2 += bleu2\n",
    "\n",
    "print(\"Average Results over 3 iterations\")\n",
    "print(\"v13  ---------------\")\n",
    "print(f\"Average BLEU Score: {model1/5:.2f}\")\n",
    "print(\"\")\n",
    "print(\"v8  ---------------\")\n",
    "print(f\"Average BLEU Score: {model2/5:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

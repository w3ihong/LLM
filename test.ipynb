{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, system_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n",
      "Skipping invalid line:  - Error: Expecting value: line 2 column 1 (char 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15952\\3783565988.py:29: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "# Load JSONL data\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line: {line.strip()} - Error: {e}\")\n",
    "                continue  # Skip malformed lines\n",
    "\n",
    "    return data\n",
    "\n",
    "rag_jsonl_path = \"./training_data/raw_data/comb.jsonl\" \n",
    "\n",
    "# Example: JSONL structure\n",
    "# {\"question\": \"What is RAG?\", \"answer\": \"Retrieval-Augmented Generation...\"}\n",
    "jsonl_data = load_jsonl(rag_jsonl_path)\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Convert data into LangChain Document format\n",
    "documents = [Document(page_content=item[\"input\"], metadata={\"output\": item[\"output\"]}) for item in jsonl_data]\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embed_model)\n",
    "\n",
    "# Save FAISS index\n",
    "vector_store.save_local(\"faiss_knowledge_base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index later\n",
    "vector_store = FAISS.load_local(\"faiss_knowledge_base\", embed_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, top_k=3, threshold=0.5):\n",
    "    docs = vector_store.similarity_search_with_score(query, top_k)\n",
    "\n",
    "    results = \"\"\n",
    "\n",
    "    for doc in docs:\n",
    "        if doc[1] < threshold:\n",
    "            results += f\" {doc[0].metadata['output']}\\n\"\n",
    "\n",
    "\n",
    "    return results if results else \"NIL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_response_w_RAG(model, tokenizer, prompt):\n",
    "\n",
    "    rejection_messages = [\n",
    "    \"C'mon now. I'm not ChatGPT or Google.\",\n",
    "    \"No clue, try somewhere else\",\n",
    "    \"I have no idea, go search online.\",\n",
    "    \"Hey, I'm not a search engine.\",\n",
    "    \"I'm not sure, try a friend.\",\n",
    "    ]\n",
    "\n",
    "    results = retrieve_docs(prompt)\n",
    "    if results == \"NIL\":\n",
    "        return random.choice(rejection_messages)\n",
    "    \n",
    "\n",
    "    rag_prompt = \"The user is asking for information about Wei Hong, you are to respond as him. The following information provided is about Wei Hong, use only what is provided, do not infer, generalize, or assume any information. If no relavant information is provided, respond with 'I don't know':{}\".format(results)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rag_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "def filter_response(response, context):\n",
    "    \"\"\"\n",
    "    Ensures that the response strictly adheres to the retrieved context.\n",
    "    If the response introduces unverified data, return a rejection message.\n",
    "    \"\"\"\n",
    "    # ðŸ”¹ Token Matching: Ensure response words exist in context\n",
    "    context_words = set(context.lower().split())\n",
    "    response_words = set(response.lower().split())\n",
    "    \n",
    "    if not response_words.issubset(context_words):\n",
    "        return random.choice(rejection_messages)\n",
    "\n",
    "    # ðŸ”¹ Semantic Similarity Check\n",
    "    context_embedding = embed_model.encode(context, convert_to_tensor=True)\n",
    "    response_embedding = embed_model.encode(response, convert_to_tensor=True)\n",
    "    similarity_score = util.pytorch_cos_sim(response_embedding, context_embedding).item()\n",
    "\n",
    "    # If similarity is low, reject response\n",
    "    if similarity_score < 0.75:  # Adjust threshold based on testing\n",
    "        return random.choice(rejection_messages)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"./trained_models/1.5b-v13\"\n",
    "model, tokenizer = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My ultimate goals include creating meaningful AI-driven solutions, transitioning into a solutions architect role, and potentially starting my own tech company.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What goals do you have in the future career wise?\"\n",
    "model_output = generate_response_w_RAG(model, tokenizer, input_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! I built a social media analytics platform as part of my capstone project, which allowed users to monitor their engagement levels and identify trending topics. Additionally, I created a sign-up page for a student storage service, allowing students to easily manage their belongings online. Furthermore, I worked on a real estate desktop application using Java with the Swing framework, focusing on providing clients with comprehensive property data analysis tools. Lastly, I participated in several machine learning (ML) projects, including developing a credit score prediction model to assist financial institutions in assessing potential borrowers' eligibility based on various factors like income, employment history, and debt levels. Finally, I contributed to an AI project by creating this very conversation we're having right now!\n"
     ]
    }
   ],
   "source": [
    "input_text = \"tell me about the projects you have done?\"\n",
    "model_output = generate_response_w_RAG(model, tokenizer, input_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W3i Solutions is a student summer storage service that was established by myself to offer affordable and accessible storage options specifically designed for students who need temporary housing during their dorm move-out periods. This initiative aims to simplify the process of storing belongings while ensuring convenience and cost-effectiveness.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Tell me about w3i solutions\"\n",
    "model_output = generate_response_w_RAG(model, tokenizer, input_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, I'm not a search engine.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Who is the president of the USA?\"\n",
    "model_output = generate_response_w_RAG(model, tokenizer, input_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI will greatly impact many industries, especially those where human-to-human interaction is less crucial. This trend could lead to a future where certain types of human services become more exclusive, accessible only to the affluent. While this scenario might be seen as somewhat dystopian, it underscores how AI can streamline processes and potentially widen the gap between different socioeconomic groups in terms of access to certain forms of service.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What are your thoughts on the future of AI?\"\n",
    "model_output = generate_response_w_RAG(model, tokenizer, input_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_eval_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                continue\n",
    "\n",
    "    return data\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_path):\n",
    "\n",
    "    eval_data = load_eval_data(eval_path)\n",
    "\n",
    "    # Load Metrics\n",
    "    bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "\n",
    "    # Evaluate Performance\n",
    "    total_count = len(eval_data)\n",
    "    exact_match_count = 0\n",
    "    bleu_scores = []\n",
    "\n",
    "    test_log = {}\n",
    "\n",
    "    for example in eval_data:\n",
    "        input_text = example[\"input\"]\n",
    "        expected_output = example[\"output\"]\n",
    "\n",
    "        # Generate model response\n",
    "        model_output = generate_response_w_RAG(model, tokenizer, input_text)\n",
    "\n",
    "        # Exact match\n",
    "        if model_output.strip().lower() == expected_output.strip().lower():\n",
    "            exact_match_count += 1\n",
    "\n",
    "        # BLEU score\n",
    "        bleu_score = bleu_metric.compute(predictions=[model_output], references=[[expected_output]])[\"score\"]\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        test_log[input_text] = {\n",
    "            \"expected\": expected_output,\n",
    "            \"generated\": model_output,\n",
    "            \"bleu_score\": bleu_score\n",
    "        }\n",
    "\n",
    "    # Compute Final Metrics\n",
    "    accuracy = exact_match_count / total_count\n",
    "    average_bleu = sum(bleu_scores) / total_count\n",
    "\n",
    "    print(\"\\nðŸ“ˆ **Final Evaluation Results:**\")\n",
    "    print(f\"ðŸŽ¯ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"ðŸ“Š Average BLEU Score: {average_bleu:.2f}\")\n",
    "    return test_log, accuracy, average_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_1 = \"./trained_models/0.5b-v13\"\n",
    "model_path_2 = \"./trained_models/0.5b-v8\"\n",
    "# model_path_3 = \"./trained_models/0.5b-v6\"\n",
    "# base_model_path = \"./base_models/Qwen2.5-0.5B-inst\"\n",
    "EVAL_FILE = \"./training_data/raw_data/comb.jsonl\"  \n",
    "SYSTEM_PROMPT = \"The user is asking for information about Wei Hong, you are to respond as him. Use only data that you are trained on, do not infer, generalize, or assume any information. If you do not know the answer, respond with 'I don't know'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_1, tokenizer_1 = load_model(model_path_1)\n",
    "model_2, tokenizer_2 = load_model(model_path_2)\n",
    "# model_3, tokenizer_3 = load_model(model_path_3)\n",
    "# bmodel, btokenizer = load_model(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration 1 \n",
      "\n",
      "v13  ---------------\n",
      "v8  ---------------\n",
      " iteration 2 \n",
      "\n",
      "v13  ---------------\n",
      "v8  ---------------\n",
      " iteration 3 \n",
      "\n",
      "v13  ---------------\n",
      "v8  ---------------\n",
      "Average Results over 3 iterations\n",
      "v13  ---------------\n",
      "Average BLEU Score: 19.75\n",
      "\n",
      "v8  ---------------\n",
      "Average BLEU Score: 21.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"v8  ---------------\")\n",
    "# log1, accuracy1, bleu1 = evaluate_model(model_1, tokenizer_1, EVAL_FILE)\n",
    "# print(\"\")\n",
    "# print(\"v12  ---------------\")\n",
    "# log2, accuracy2, bleu2 = evaluate_model(model_2, tokenizer_2, EVAL_FILE)\n",
    "# print(\"\")\n",
    "# print(\"v6  ---------------\")\n",
    "# log3, accuracy3, bleu3 = evaluate_model(model_3, tokenizer_3, EVAL_FILE)\n",
    "# print(\"\")\n",
    "# print(\"Base model ---------------\")\n",
    "# logb, accuracyb, bleub = evaluate_model(bmodel, btokenizer, EVAL_FILE)\n",
    "\n",
    "model1 = 0\n",
    "model2 = 0 \n",
    "for i in range (3):\n",
    "    print(\" iteration\", i+1 )\n",
    "    print(\"v13  ---------------\")\n",
    "    log1, accuracy1, bleu1 = evaluate_model(model_1, tokenizer_1, EVAL_FILE)\n",
    "    print(\"v8  ---------------\")\n",
    "    log2, accuracy2, bleu2 = evaluate_model(model_2, tokenizer_2, EVAL_FILE)\n",
    "    model1 += bleu1\n",
    "    model2 += bleu2\n",
    "\n",
    "print(\"Average Results over 3 iterations\")\n",
    "print(\"v13  ---------------\")\n",
    "print(f\"Average BLEU Score: {model1/5:.2f}\")\n",
    "print(\"\")\n",
    "print(\"v8  ---------------\")\n",
    "print(f\"Average BLEU Score: {model2/5:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
